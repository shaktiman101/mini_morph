vocab_size: 51936           # Vocabulary size
context_length: 1024        # Context length that was used to train the model
max_len: 1024              # Maximum length of the input sequence
emb_dim: 1008                # Embedding dimension
n_heads: 12                 # Number of attention heads
n_layers: 12                  # Number of layers
hidden_dim: 1536              # Size of the intermediate dimension in FeedForward
head_dim: 128               # Size of the heads in GQA
qk_norm: True                 # Whether to normalize queries and values in GQA
n_kv_groups: 8                # Key-Value groups for grouped-query attention
rope_base: 1000000.0        # The base in RoPE's theta
dtype: bfloat16         # Lower-precision dtype to reduce memory usage
qkv_bias: False
dropout: 0.0                 # Dropout rate
training: True                # Whether the model is in training mode
device: cuda                 # Device to run the model on