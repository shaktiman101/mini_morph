model_type: llama2
vocab_size: 32000
max_position_embeddings: 4096          # LLaMA 2 base sequence length
rope_theta: 10000.0                   # Base RoPE period
hidden_act: silu                      # SwiGLU nonlinearity
norm_type: RMSNorm                   # Normalization
autoregressive: true
architecture: decoder-only transformer

shared:
  initializer_range: 0.02
  rms_norm_eps: 1e-6
  use_cache: true
  pad_token_id: null
  bos_token_id: 1
  eos_token_id: 2

rope_scaling: null  # default ("default" RoPE). For extended context use `rope_scaling: { type: "linear", factor: x }`

sizes:
  7B:
    layers: 32
    hidden_size: 4096
    intermediate_size: 11008
    num_attention_heads: 32
    num_key_value_heads: 32        # full MHA
    attention_type: MHA
    parameter_count: ~6.9e9

  13B:
    layers: 40
    hidden_size: 5120
    intermediate_size: 13824
    num_attention_heads: 40
    num_key_value_heads: 40        # still MHA
    attention_type: MHA
    parameter_count: ~13e9

  70B:
    layers: 80
    hidden_size: 8192
    intermediate_size: 28672
    num_attention_heads: 64
    num_key_value_heads: 8         # 8‑way GQA (meta reported GQA on 70B) :contentReference[oaicite:2]{index=2}
    attention_type: GQA
    parameter_count: ~70e9
